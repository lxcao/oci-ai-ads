{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>Oracle Cloud Infrastructure Data Science Sample Notebook\n",
    "\n",
    "Copyright (c) 2021 Oracle, Inc.  All rights reserved. <br>\n",
    "Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying a PyTorch Model with Model Deployment \n",
    "\n",
    "In this tutorial we are going to prepare and save a pytorch model artifact using ADS, we are going to publish a conda environment, and deploy the model as an HTTP endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites to Running this Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We recommend that you run this notebook in a notebook session using the **Data Science Conda Environment \"General Machine Learning for CPU (v1.0)\"** \n",
    "* You need access to the public internet\n",
    "* **You need to upgrade the current version of the OCI Python SDK** (`oci`)\n",
    "* You need to install the `transformers` library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade oci\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "import ads\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "ads.set_documentation_mode(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download a pre-trained bert model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model\n",
    "pretrained_model_name = \"lannelin/bert-imdb-1hidden\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we installed `transformers` in our conda environment, let's first publish the environment before saving the model to the catalog. We will need the same environment (with `transformers`) for model deployment. \n",
    "\n",
    "You can publish an environment by first initializing `odsc conda` with the namespace of your tenancy and the object storage bucket name where you want to store the conda environment. Then execute the `odsc conda publish` command in the terminal to copy the environment in the bucket. This command can take a few minutes to complete: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!odsc conda init -b <your-bucket-name>  -n <your-tenancy-namespace> # replace with your values. \n",
    "!odsc conda publish -s mlcpuv1 # change this value if you are running this notebook in a different conda environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also make sure that you write a policy allowing model deployment to read objects in your bucket: \n",
    "\n",
    "```\n",
    "Allow any-user to read objects in compartment <your-compartment-name>\n",
    "where ALL { request.principal.type='datasciencemodeldeployment', \n",
    "target.bucket.name=<your-bucket-name> }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are done publishing the environment, we need to provide a reference of its path on object storage. The path of a published conda environment should be passsed to the parameter `inference_conda_env`. \n",
    "\n",
    "If you don't know how to find the path of your environment on object storage, simply go back to the \"Environment Explorer\" tool in the notebook session. Click on \"Published Environments\". The path is written on each environment card (`Object Storage URI`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the inference conda environment.\n",
    "inference_conda_env = \"<your-conda-env-object-storage-path>\" # replace with your value. \n",
    "\n",
    "# Prepare the model artifact template\n",
    "path_to_model_artifacts = \"pytorch_artifacts\"\n",
    "model_artifact = prepare_generic_model(path_to_model_artifacts,\n",
    "                                               function_artifacts=False,\n",
    "                                               force_overwrite=True,\n",
    "                                               data_science_env=False,\n",
    "                                               inference_conda_env=inference_conda_env)\n",
    "model.save_pretrained(path_to_model_artifacts)\n",
    "tokenizer.save_pretrained(path_to_model_artifacts)\n",
    "\n",
    "# List the template files\n",
    "print(\"Model Artifact Path: {}\\n\\nModel Artifact Files:\".format(\n",
    "    path_to_model_artifacts))\n",
    "for file in os.listdir(path_to_model_artifacts):\n",
    "    if path.isdir(path.join(path_to_model_artifacts, file)):\n",
    "        for file2 in os.listdir(path.join(path_to_model_artifacts, file)):\n",
    "            print(path.join(file, file2))\n",
    "    else:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "score = '''\n",
    "import json\n",
    "import os\n",
    "\n",
    "from functools import lru_cache\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"pytorch_model.bin\"\n",
    "tokenize_name = 'vocab'\n",
    "\n",
    "@lru_cache(maxsize=10)\n",
    "def load_model(model_file_name=model_name):\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    model_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    contents = os.listdir(model_dir)\n",
    "    if model_file_name in contents:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        return model\n",
    "    else:\n",
    "        raise Exception('{0} is not found in model directory {1}'.format(model_file_name, model_dir))\n",
    "\n",
    "\n",
    "def predict(data, model=load_model()):\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Panda DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: {'prediction':output from model.predict method}\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    contents = os.listdir(tokenizer_dir)\n",
    "    LABELS = [\"negative\", \"positive\"]    \n",
    "    if tokenize_name + '.json' in contents:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "    outputs = []\n",
    "    for text in data:\n",
    "        inputs = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "        output = model(**inputs)[0].squeeze().detach().numpy()\n",
    "        outputs.append(LABELS[(output.argmax())])\n",
    "    return {'prediction': outputs}\n",
    "'''\n",
    "\n",
    "with open(path.join(path_to_model_artifacts, \"score.py\"), 'w') as f:\n",
    "    print(f.write(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = os.environ['PROJECT_OCID'] \n",
    "compartment_id = os.environ['NB_SESSION_COMPARTMENT_OCID']\n",
    "\n",
    "mc_model = model_artifact.save(\n",
    "    project_id=project_id, compartment_id=compartment_id, \n",
    "    display_name=\"pytorch_model (Model Deployment Test)\",\n",
    "    description=\"A sample bert pretrained model\",\n",
    "    ignore_pending_changes=True, timeout=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print published model information\n",
    "mc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the model with Model Deployment\n",
    "\n",
    "We are ready to deploy `mc_model`. We are using the user principal (config+key) method of authentication. Alternatively you can use resource principal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting OCI config information\n",
    "oci_config = oci.config.from_file(\"~/.oci/config\", \"DEFAULT\")\n",
    "# Setting up DataScience instance\n",
    "data_science = oci.data_science.DataScienceClient(oci_config)\n",
    "# Setting up data science composite client to unlock wait_for_state operations\n",
    "data_science_composite = oci.data_science.DataScienceClientCompositeOperations(data_science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model deployment configuration object: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepareing model deployment data\n",
    "model_deployment_details = {\n",
    "    \"displayName\": \"Pytorch model test\",\n",
    "    \"projectId\": mc_model.project_id,\n",
    "    \"compartmentId\": mc_model.compartment_id,\n",
    "    \"modelDeploymentConfigurationDetails\": {\n",
    "        \"deploymentType\": \"SINGLE_MODEL\",\n",
    "        \"modelConfigurationDetails\": {\n",
    "            \"modelId\": mc_model.id,\n",
    "            \"instanceConfiguration\": {\n",
    "                \"instanceShapeName\": \"VM.Standard2.4\"\n",
    "            },\n",
    "            \"scalingPolicy\": {\n",
    "                \"policyType\": \"FIXED_SIZE\",\n",
    "                \"instanceCount\": 2\n",
    "            },\n",
    "            \"bandwidthMbps\": 10\n",
    "        }\n",
    "    },\n",
    "    \"categoryLogDetails\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to deploy. This takes a few minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model_deployment = data_science_composite.create_model_deployment_and_wait_for_state(model_deployment_details,\n",
    "                                                                                     wait_for_states=[\"SUCCEEDED\",\n",
    "                                                                                                      \"FAILED\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell extract from the `model_deployment` object a series of useful diagnostics about the creation of the model deployment resource: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grabbing the model deployment ocid...\")\n",
    "model_deployment_data = json.loads(str(model_deployment.data))\n",
    "model_deployment_id = model_deployment_data['resources'][0]['identifier']\n",
    "print(f\"Model deployment ocid: {model_deployment_id}\")\n",
    "\n",
    "print(\"Checking for the correct response status code...\")\n",
    "if model_deployment.status == 200:\n",
    "    print(f\"Work request status code returned: {model_deployment.status}\")\n",
    "    print(\"Checking for non-empty response data...\")\n",
    "    if model_deployment.data:\n",
    "        print(f\"Data returned: {model_deployment.data}\")\n",
    "        print(\"Grabbing the model deployment work request status...\")\n",
    "        work_request_status = model_deployment_data['status']\n",
    "        print(\"Checking for the correct work request status...\")\n",
    "        if work_request_status == \"SUCCEEDED\":\n",
    "            print(f\"Work request status returned: {work_request_status}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Work request returned an incorrect status of: {work_request_status}\")\n",
    "            print(\n",
    "                f\"Work requests error: {data_science.list_work_request_errors(model_deployment.data.id).data}\")\n",
    "            print(\n",
    "                f\"opc-request-id: {model_deployment.headers['opc-request-id']}\")\n",
    "    else:\n",
    "        print(\"Failed to grab model deployment data.\")\n",
    "        print(f\"opc-request-id: {model_deployment.headers['opc-request-id']}\")\n",
    "else:\n",
    "    print(\n",
    "        f\"Model deployment returned an incorrect status of: { model_deployment.status}\")\n",
    "    print(f\"opc-request-id: {model_deployment.headers['opc-request-id']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mlcpuv1]",
   "language": "python",
   "name": "conda-env-mlcpuv1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
